{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravit-cohen-segev/ravit-cohen-segev/blob/main/DnCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0mqg78lYE5N6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data as td\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from abc import ABC, abstractmethod\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "#additional packages needed for image pre-processing\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e0_3O4o6FKS1"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module, ABC):\n",
        "    \"\"\"An abstract class representing a neural network.\n",
        "    All other neural network should subclass it. All subclasses should override\n",
        "    ``forward``, that makes a prediction for its input argument, and\n",
        "    ``criterion``, that evaluates the fit between a prediction and a desired\n",
        "    output. This class inherits from ``nn.Module`` and overloads the method\n",
        "    ``named_parameters`` such that only parameters that require gradient\n",
        "    computation are returned. Unlike ``nn.Module``, it also provides a property\n",
        "    ``device`` that returns the current device in which the network is stored\n",
        "    (assuming all network parameters are stored on the same device).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        # This is important that this is a property and not an attribute as the\n",
        "        # device may change anytime if the user do ``net.to(newdevice)``.\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def named_parameters(self, recurse=True):\n",
        "        nps = nn.Module.named_parameters(self)\n",
        "        for name, param in nps:\n",
        "            if not param.requires_grad:\n",
        "                continue\n",
        "            yield name, param\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def criterion(self, y, d):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uJpTNaDvFQ2l"
      },
      "outputs": [],
      "source": [
        "class ResidualDenoiser(NeuralNetwork):\n",
        "  def __init__(self, in_channels=3 , num_features=64, expansions=18):\n",
        "        super().__init__()\n",
        "        # build the network consisting of:\n",
        "        # zero padding to input\n",
        "        # first layer - conv2d with filters (3,3,1)\n",
        "        # create block template for residual blocks: conv2d -> BatchNormaliztion ->relu\n",
        "        # last layer - conv2d layer\n",
        "        all_layers = []\n",
        "\n",
        "        first_layer = [nn.Conv2d(in_channels=in_channels, out_channels=num_features, kernel_size=(3,3), padding=1),\n",
        "                                         nn.ReLU()]\n",
        "\n",
        "        block = [nn.Conv2d(in_channels=num_features, out_channels=num_features, kernel_size=(3,3), padding=1),\n",
        "                                   nn.BatchNorm2d(num_features),\n",
        "                                   nn.ReLU()]\n",
        "        bottleneck = []\n",
        "        for i in range(expansions):\n",
        "            bottleneck += block\n",
        "\n",
        "        last_layer = [nn.Conv2d(in_channels=num_features, out_channels=in_channels, kernel_size=(3,3), padding=1)]\n",
        "\n",
        "        #add all layers in order to all_list and then convert to Sequential\n",
        "        all_layers += first_layer\n",
        "        all_layers += bottleneck\n",
        "        all_layers += last_layer\n",
        "\n",
        "        self.model = nn.Sequential(*all_layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "     x = self.model(x)\n",
        "     return x\n",
        "\n",
        "  def criterion(self, y, d):\n",
        "     loss = torch.nn.MSELoss()\n",
        "     return loss(y,d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BSzc_6zSFUsV"
      },
      "outputs": [],
      "source": [
        "class Experiment(object):\n",
        "    \"\"\"\n",
        "    A class meant to run a neural network learning experiment.\n",
        "    After being instantiated, the experiment can be run using the method\n",
        "    ``run``. At each epoch, a checkpoint file will be created in the directory\n",
        "    ``output_dir``. Two files will be present: ``checkpoint.pth.tar`` a binary\n",
        "    file containing the state of the experiment, and ``config.txt`` an ASCII\n",
        "    file describing the setting of the experiment. If ``output_dir`` does not\n",
        "    exist, it will be created. Otherwise, the last checkpoint will be loaded,\n",
        "    except if the setting does not match (in that case an exception will be\n",
        "    raised). The loaded experiment will be continued from where it stopped when\n",
        "    calling the method ``run``. The experiment can be evaluated using the method\n",
        "    ``evaluate``.\n",
        "\n",
        "    Attributes/Properties:\n",
        "        epoch (integer): the number of performed epochs.\n",
        "        history (list): a list of statistics for each epoch.\n",
        "\n",
        "    Arguments:\n",
        "        net (NeuralNetork): a neural network.\n",
        "        train_set (Dataset): a training data set.\n",
        "        val_set (Dataset): a validation data set.\n",
        "        output_dir (string, optional): path where to load/save checkpoints. If\n",
        "            None, ``output_dir`` is set to \"experiment_TIMESTAMP\" where\n",
        "            TIMESTAMP is the current time stamp as returned by ``time.time()``.\n",
        "            (default: None)\n",
        "        batch_size (integer, optional): the size of the mini batches.\n",
        "            (default: 16)\n",
        "        perform_validation_during_training (boolean, optional): if False,\n",
        "            statistics at each epoch are computed on the training set only.\n",
        "            If True, statistics at each epoch are computed on both the training\n",
        "            set and the validation set. (default: False)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, net, train_set, val_set, optimizer,\n",
        "                 output_dir=None, batch_size=1, perform_validation_during_training=True):\n",
        "\n",
        "        self.train_loader = train_set\n",
        "        self.val_loader = val_set\n",
        "\n",
        "        # Initialize epochs\n",
        "        self.epoch = 0\n",
        "\n",
        "        # Define checkpoint paths\n",
        "        if output_dir is None:\n",
        "            output_dir = 'experiment_{}'.format(time.time())\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        checkpoint_path = os.path.join(output_dir, \"checkpoint.pth.tar\")\n",
        "        config_path = os.path.join(output_dir, \"config.txt\")\n",
        "\n",
        "        # Transfer all local arguments/variables into attributes\n",
        "        locs = {k: v for k, v in locals().items() if k is not 'self'}\n",
        "        self.__dict__.update(locs)\n",
        "\n",
        "        # Load checkpoint and check compatibility\n",
        "        if os.path.isfile(config_path):\n",
        "            with open(config_path, 'r') as f:\n",
        "                if f.read()[:-1] != repr(self):\n",
        "                    raise ValueError(\n",
        "                        \"Cannot create this experiment: \"\n",
        "                        \"I found a checkpoint conflicting with the current setting.\")\n",
        "            self.load()\n",
        "        else:\n",
        "            self.save()\n",
        "\n",
        "    def setting(self):\n",
        "        \"\"\"Returns the setting of the experiment.\"\"\"\n",
        "        return {'Net': self.net.double(),\n",
        "                'TrainSet': self.train_set,\n",
        "                'ValSet': self.val_set,\n",
        "                'Optimizer': self.optimizer,\n",
        "                'BatchSize': self.batch_size,\n",
        "                'PerformValidationDuringTraining': self.perform_validation_during_training}\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Pretty printer showing the setting of the experiment. This is what\n",
        "        is displayed when doing ``print(experiment)``. This is also what is\n",
        "        saved in the ``config.txt`` file.\n",
        "        \"\"\"\n",
        "        string = ''\n",
        "        for key, val in self.setting().items():\n",
        "            string += '{}({})\\n'.format(key, val)\n",
        "        return string\n",
        "\n",
        "    def state_dict(self):\n",
        "        \"\"\"Returns the current state of the experiment.\"\"\"\n",
        "        return {'Net': self.net.state_dict(),\n",
        "                'Optimizer': self.optimizer.state_dict(),\n",
        "                'Epoch': self.epoch}\n",
        "\n",
        "    def load_state_dict(self, checkpoint):\n",
        "        \"\"\"Loads the experiment from the input checkpoint.\"\"\"\n",
        "        self.net.load_state_dict(checkpoint['Net'])\n",
        "        self.optimizer.load_state_dict(checkpoint['Optimizer'])\n",
        "        self.epoch = checkpoint['Epoch']\n",
        "\n",
        "        # The following loops are used to fix a bug that was\n",
        "        # discussed here: https://github.com/pytorch/pytorch/issues/2830\n",
        "        # (it is supposed to be fixed in recent PyTorch version)\n",
        "        for state in self.optimizer.state.values():\n",
        "            for k, v in state.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    state[k] = v.to(self.net.device)\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\"Saves the experiment on disk, i.e, create/update the last checkpoint.\"\"\"\n",
        "        torch.save(self.state_dict(), self.checkpoint_path)\n",
        "        with open(self.config_path, 'w') as f:\n",
        "            print(self, file=f)\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"Loads the experiment from the last checkpoint saved on disk.\"\"\"\n",
        "        checkpoint = torch.load(self.checkpoint_path,\n",
        "                                map_location=self.net.device)\n",
        "        self.load_state_dict(checkpoint)\n",
        "        del checkpoint\n",
        "\n",
        "    def run(self, num_epochs):\n",
        "        \"\"\"Runs the experiment, i.e., trains the network using backpropagation\n",
        "        based on the optimizer and the training set. Adds loss and epoch to Tensorboard dashboards\"\n",
        "\n",
        "        Arguments:\n",
        "            num_epoch (integer): the number of epoch to perform.\n",
        "        \"\"\"\n",
        "        self.net.train()\n",
        "        start_epoch = self.epoch\n",
        "        print(\"Start/Continue training from epoch {}\".format(start_epoch))\n",
        "\n",
        "        #For Tensorboard\n",
        "        writer = SummaryWriter('logs/Tensorboard_exp')\n",
        "        for epoch in range(start_epoch, num_epochs):\n",
        "            s = time.time()\n",
        "            for x, d in self.train_loader:\n",
        "                x,d =x.type(torch.DoubleTensor), d.type(torch.DoubleTensor)\n",
        "                x, d = x.to(self.net.device), d.to(self.net.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                y = self.net.forward(x)\n",
        "                loss = self.net.criterion(y, d)\n",
        "                writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "            #perform validation\n",
        "            self.evaluate(writer)\n",
        "            print(\"Epoch {} (Time: {:.2f}s)\".format(\n",
        "                self.epoch, time.time() - s))\n",
        "            self.epoch +=1\n",
        "            self.save()\n",
        "        writer.flush()\n",
        "        print(\"Finish training for {} epochs\".format(num_epochs))\n",
        "\n",
        "    def evaluate(self, writer):\n",
        "        \"\"\"Evaluates the experiment, i.e., forward propagates the validation set\n",
        "        through the network and add loss and epoch to Tensoboard\"\n",
        "        \"\"\"\n",
        "        self.net.eval()\n",
        "      \n",
        "        with torch.no_grad():\n",
        "            for x, d in self.val_loader:\n",
        "                x,d =x.type(torch.DoubleTensor), d.type(torch.DoubleTensor)\n",
        "                x, d = x.to(self.net.device), d.to(self.net.device)\n",
        "                y = self.net.forward(x)\n",
        "                loss = self.net.criterion(y, d)\n",
        "                writer.add_scalar(\"Loss/val\", loss, self.epoch)\n",
        "        self.net.train()\n",
        "        return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7HjgYYcwFdsF"
      },
      "outputs": [],
      "source": [
        "def GaussianNoise(arr, mean=0., std=0.1):\n",
        "    'adds gaussian noise to array'\n",
        "    return np.random.normal(mean, std, arr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OFU_pJxHFf2V"
      },
      "outputs": [],
      "source": [
        "class Custom_Dataset():  \n",
        "    def __init__(self, path):\n",
        "       self.path = path   \n",
        "       self.train_noisy, self.train_residual = self.__getitem__('train')\n",
        "       self.val_noisy, self.val_residual = self.__getitem__('val')\n",
        "       self.max_height = max([img.shape[1] for img in [*self.train_noisy, *self.val_noisy]])\n",
        "       self.max_width = max([img.shape[2] for img in [*self.train_noisy, *self.val_noisy]])\n",
        "       \n",
        "       #pad the images\n",
        "       self.train_batch = list(zip(self.__padding__(self.train_noisy), self.__padding__(self.train_residual)))\n",
        "       self.val_batch = list(zip(self.__padding__(self.val_noisy), self.__padding__(self.val_residual)))\n",
        "           \n",
        "    def __getitem__(self, folder_name):\n",
        "        full_path = os.path.join(self.path, folder_name)\n",
        "        file_names = os.listdir(full_path)\n",
        "        noisy_images = [] #input\n",
        "        residual_noise = [] #output\n",
        "        \n",
        "        for file in file_names:\n",
        "            file_path = os.path.join(full_path, file)\n",
        "            image = Image.open(file_path)\n",
        "            \n",
        "            #convert to numpy\n",
        "            image = np.asarray(image)\n",
        "            \n",
        "            #images.append(TF.to_tensor(image))\n",
        "            gaus_noise = GaussianNoise(image)   \n",
        "            \n",
        "            residual_noise.append(transforms.ToTensor()(gaus_noise))\n",
        "            noisy_image = image + gaus_noise\n",
        "            noisy_images.append(transforms.ToTensor()(noisy_image))\n",
        "        return noisy_images, residual_noise\n",
        "\n",
        "    def __padding__(self, batch):               \n",
        "        image_batch = [\n",
        "        # The needed padding is the difference between the\n",
        "        # max width/height and the image's actual width/height.\n",
        "        F.pad(img, [0, self.max_width - img.shape[2], 0, self.max_height - img.shape[1]])\n",
        "        for img in batch]\n",
        "        return image_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_6xThUtEv0KD"
      },
      "outputs": [],
      "source": [
        "def remove_noise(noisy_image, noise):\n",
        "  '''input: two tensors. noisy image tensor and noise tensor\n",
        "  plots denoised image'''\n",
        "  #remove noise and reduce output tensor to 3 dimensions\n",
        "  image_tensor = torch.sub(noisy_image, noise).squeeze()\n",
        "  \n",
        "  #plot image\n",
        "  plt.imshow(transforms.ToPILImage()(image_tensor))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obKLY3GFGrvX"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRwSUDliFnQO",
        "outputId": "8c7766d8-2edb-44d6-c5f8-81cccaf9f34a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC2DmLJlFsf-",
        "outputId": "92ef950d-5f2e-4eac-88f8-04f660890f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/smaller_dataset.zip\n",
            "   creating: smaller_dataset/\n",
            "   creating: smaller_dataset/train/\n",
            "  inflating: smaller_dataset/train/100075.jpg  \n",
            "  inflating: smaller_dataset/train/100080.jpg  \n",
            "  inflating: smaller_dataset/train/100098.jpg  \n",
            "  inflating: smaller_dataset/train/103041.jpg  \n",
            "  inflating: smaller_dataset/train/104022.jpg  \n",
            "  inflating: smaller_dataset/train/105019.jpg  \n",
            "  inflating: smaller_dataset/train/105053.jpg  \n",
            "  inflating: smaller_dataset/train/106020.jpg  \n",
            "  inflating: smaller_dataset/train/106025.jpg  \n",
            "  inflating: smaller_dataset/train/108041.jpg  \n",
            "  inflating: smaller_dataset/train/108073.jpg  \n",
            "  inflating: smaller_dataset/train/109034.jpg  \n",
            "  inflating: smaller_dataset/train/112082.jpg  \n",
            "  inflating: smaller_dataset/train/113009.jpg  \n",
            "  inflating: smaller_dataset/train/113016.jpg  \n",
            "  inflating: smaller_dataset/train/113044.jpg  \n",
            "  inflating: smaller_dataset/train/117054.jpg  \n",
            "  inflating: smaller_dataset/train/118020.jpg  \n",
            "  inflating: smaller_dataset/train/118035.jpg  \n",
            "  inflating: smaller_dataset/train/12003.jpg  \n",
            "  inflating: smaller_dataset/train/12074.jpg  \n",
            "  inflating: smaller_dataset/train/122048.jpg  \n",
            "  inflating: smaller_dataset/train/124084.jpg  \n",
            "  inflating: smaller_dataset/train/126039.jpg  \n",
            "  inflating: smaller_dataset/train/130034.jpg  \n",
            "  inflating: smaller_dataset/train/134008.jpg  \n",
            "  inflating: smaller_dataset/train/134052.jpg  \n",
            "  inflating: smaller_dataset/train/135037.jpg  \n",
            "  inflating: smaller_dataset/train/135069.jpg  \n",
            "  inflating: smaller_dataset/train/138032.jpg  \n",
            "  inflating: smaller_dataset/train/138078.jpg  \n",
            "  inflating: smaller_dataset/train/140055.jpg  \n",
            "  inflating: smaller_dataset/train/140075.jpg  \n",
            "  inflating: smaller_dataset/train/144067.jpg  \n",
            "  inflating: smaller_dataset/train/145014.jpg  \n",
            "  inflating: smaller_dataset/train/145053.jpg  \n",
            "  inflating: smaller_dataset/train/147021.jpg  \n",
            "  inflating: smaller_dataset/train/147062.jpg  \n",
            "  inflating: smaller_dataset/train/15004.jpg  \n",
            "  inflating: smaller_dataset/train/15088.jpg  \n",
            "  inflating: smaller_dataset/train/151087.jpg  \n",
            "  inflating: smaller_dataset/train/16052.jpg  \n",
            "  inflating: smaller_dataset/train/20008.jpg  \n",
            "  inflating: smaller_dataset/train/2092.jpg  \n",
            "  inflating: smaller_dataset/train/22013.jpg  \n",
            "  inflating: smaller_dataset/train/22090.jpg  \n",
            "  inflating: smaller_dataset/train/22093.jpg  \n",
            "  inflating: smaller_dataset/train/23025.jpg  \n",
            "  inflating: smaller_dataset/train/23080.jpg  \n",
            "  inflating: smaller_dataset/train/23084.jpg  \n",
            "  inflating: smaller_dataset/train/24004.jpg  \n",
            "  inflating: smaller_dataset/train/24063.jpg  \n",
            "  inflating: smaller_dataset/train/25098.jpg  \n",
            "  inflating: smaller_dataset/train/26031.jpg  \n",
            "  inflating: smaller_dataset/train/27059.jpg  \n",
            "  inflating: smaller_dataset/train/28075.jpg  \n",
            "  inflating: smaller_dataset/train/28096.jpg  \n",
            "  inflating: smaller_dataset/train/33066.jpg  \n",
            "  inflating: smaller_dataset/train/35008.jpg  \n",
            "  inflating: smaller_dataset/train/35010.jpg  \n",
            "  inflating: smaller_dataset/train/35058.jpg  \n",
            "  inflating: smaller_dataset/train/35070.jpg  \n",
            "  inflating: smaller_dataset/train/35091.jpg  \n",
            "  inflating: smaller_dataset/train/41004.jpg  \n",
            "  inflating: smaller_dataset/train/41025.jpg  \n",
            "  inflating: smaller_dataset/train/42044.jpg  \n",
            "  inflating: smaller_dataset/train/42078.jpg  \n",
            "  inflating: smaller_dataset/train/43070.jpg  \n",
            "  inflating: smaller_dataset/train/43083.jpg  \n",
            "  inflating: smaller_dataset/train/45077.jpg  \n",
            "  inflating: smaller_dataset/train/46076.jpg  \n",
            "  inflating: smaller_dataset/train/48055.jpg  \n",
            "  inflating: smaller_dataset/train/54005.jpg  \n",
            "  inflating: smaller_dataset/train/55067.jpg  \n",
            "  inflating: smaller_dataset/train/55075.jpg  \n",
            "  inflating: smaller_dataset/train/56028.jpg  \n",
            "  inflating: smaller_dataset/train/59078.jpg  \n",
            "  inflating: smaller_dataset/train/60079.jpg  \n",
            "  inflating: smaller_dataset/train/61060.jpg  \n",
            "  inflating: smaller_dataset/train/61086.jpg  \n",
            "  inflating: smaller_dataset/train/65010.jpg  \n",
            "  inflating: smaller_dataset/train/65019.jpg  \n",
            "  inflating: smaller_dataset/train/65074.jpg  \n",
            "  inflating: smaller_dataset/train/65132.jpg  \n",
            "  inflating: smaller_dataset/train/66039.jpg  \n",
            "  inflating: smaller_dataset/train/66075.jpg  \n",
            "  inflating: smaller_dataset/train/67079.jpg  \n",
            "  inflating: smaller_dataset/train/68077.jpg  \n",
            "  inflating: smaller_dataset/train/71046.jpg  \n",
            "  inflating: smaller_dataset/train/76002.jpg  \n",
            "  inflating: smaller_dataset/train/78019.jpg  \n",
            "  inflating: smaller_dataset/train/80099.jpg  \n",
            "  inflating: smaller_dataset/train/8049.jpg  \n",
            "  inflating: smaller_dataset/train/8143.jpg  \n",
            "  inflating: smaller_dataset/train/87065.jpg  \n",
            "  inflating: smaller_dataset/train/90076.jpg  \n",
            "  inflating: smaller_dataset/train/92059.jpg  \n",
            "  inflating: smaller_dataset/train/94079.jpg  \n",
            "  inflating: smaller_dataset/train/95006.jpg  \n",
            "  inflating: smaller_dataset/train/97017.jpg  \n",
            "   creating: smaller_dataset/val/\n",
            "  inflating: smaller_dataset/val/277095.jpg  \n",
            "  inflating: smaller_dataset/val/285036.jpg  \n",
            "  inflating: smaller_dataset/val/286092.jpg  \n",
            "  inflating: smaller_dataset/val/292066.jpg  \n",
            "  inflating: smaller_dataset/val/293029.jpg  \n",
            "  inflating: smaller_dataset/val/299091.jpg  \n",
            "  inflating: smaller_dataset/val/301007.jpg  \n",
            "  inflating: smaller_dataset/val/302003.jpg  \n",
            "  inflating: smaller_dataset/val/309004.jpg  \n",
            "  inflating: smaller_dataset/val/310007.jpg  \n",
            "  inflating: smaller_dataset/val/311068.jpg  \n",
            "  inflating: smaller_dataset/val/311081.jpg  \n",
            "  inflating: smaller_dataset/val/314016.jpg  \n",
            "  inflating: smaller_dataset/val/317080.jpg  \n",
            "  inflating: smaller_dataset/val/323016.jpg  \n",
            "  inflating: smaller_dataset/val/326038.jpg  \n",
            "  inflating: smaller_dataset/val/353013.jpg  \n",
            "  inflating: smaller_dataset/val/361084.jpg  \n",
            "  inflating: smaller_dataset/val/365025.jpg  \n",
            "  inflating: smaller_dataset/val/365073.jpg  \n",
            "  inflating: smaller_dataset/val/368016.jpg  \n",
            "  inflating: smaller_dataset/val/368078.jpg  \n",
            "  inflating: smaller_dataset/val/370036.jpg  \n",
            "  inflating: smaller_dataset/val/372047.jpg  \n",
            "  inflating: smaller_dataset/val/374020.jpg  \n",
            "  inflating: smaller_dataset/val/374067.jpg  \n",
            "  inflating: smaller_dataset/val/376001.jpg  \n",
            "  inflating: smaller_dataset/val/376020.jpg  \n",
            "  inflating: smaller_dataset/val/385028.jpg  \n",
            "  inflating: smaller_dataset/val/388016.jpg  \n"
          ]
        }
      ],
      "source": [
        "#unzip data folder\n",
        "!unzip /content/smaller_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Nb2VvbcqFvaX"
      },
      "outputs": [],
      "source": [
        "path = '/content/smaller_dataset'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0fpJgCPFzjG",
        "outputId": "73a0e0c7-1923-412a-cb94-f97797b8eefe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResidualDenoiser(\n",
              "  (model): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ReLU()\n",
              "    (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): ReLU()\n",
              "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU()\n",
              "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (13): ReLU()\n",
              "    (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (16): ReLU()\n",
              "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (19): ReLU()\n",
              "    (20): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (21): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (22): ReLU()\n",
              "    (23): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (25): ReLU()\n",
              "    (26): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (28): ReLU()\n",
              "    (29): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (30): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (31): ReLU()\n",
              "    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (34): ReLU()\n",
              "    (35): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (36): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (37): ReLU()\n",
              "    (38): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (39): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (40): ReLU()\n",
              "    (41): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (42): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (43): ReLU()\n",
              "    (44): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (46): ReLU()\n",
              "    (47): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (48): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (49): ReLU()\n",
              "    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (52): ReLU()\n",
              "    (53): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (54): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (55): ReLU()\n",
              "    (56): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "net = ResidualDenoiser()\n",
        "params = net.parameters()\n",
        "net.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r3dhlcJxF1hP"
      },
      "outputs": [],
      "source": [
        "custom_data = Custom_Dataset(path)\n",
        "train_data, val_data = custom_data.train_batch, custom_data.val_batch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IcAbVdqlF5En"
      },
      "outputs": [],
      "source": [
        "train_set = td.DataLoader(train_data, batch_size=1, shuffle=True)\n",
        "val_set = td.DataLoader(val_data, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "l8AaPtpwF8V_"
      },
      "outputs": [],
      "source": [
        "exp = Experiment(net, train_set=train_set, val_set=val_set, optimizer= torch.optim.SGD(params=params, lr=0.003))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "lFh4F15sWPwF"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "del train_data\n",
        "del val_data\n",
        "del train_set\n",
        "del val_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDJ8WBzUF9th",
        "outputId": "91225d8d-1db7-48d2-9051-0c8768cf7f5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start/Continue training from epoch 0\n",
            "Epoch 0 (Time: 486.08s)\n",
            "Epoch 1 (Time: 484.70s)\n",
            "Epoch 2 (Time: 486.42s)\n",
            "Epoch 3 (Time: 486.54s)\n",
            "Epoch 4 (Time: 486.52s)\n",
            "Epoch 5 (Time: 486.43s)\n"
          ]
        }
      ],
      "source": [
        "exp.run(num_epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyjLk86HGC9t"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfSBg5V5GDrV"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir '/content/logs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUwetWCw_XYp"
      },
      "outputs": [],
      "source": [
        "custom_data = Custom_Dataset(path)\n",
        "train_data, val_data = custom_data.train_batch, custom_data.val_batch \n",
        "\n",
        "val_set = td.DataLoader(val_data, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "it=iter(val_set)\n",
        "ex_im = next(it)\n",
        "squeezed_im = np.squeeze(ex_im[0])\n",
        "residual_im = np.squeeze(ex_im[1])"
      ],
      "metadata": {
        "id": "7J2Cv1StwLp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuzOxVYz_V9W"
      },
      "outputs": [],
      "source": [
        "#show performance on one of the images in the validation set \n",
        "plt.imshow(transforms.ToPILImage()(squeezed_im))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(transforms.ToPILImage()(residual_im))"
      ],
      "metadata": {
        "id": "Mb72pYAaxXQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k35FI5f4-3W4"
      },
      "outputs": [],
      "source": [
        "ex_image = ex_im[0].to('cuda')\n",
        "res_image = exp.net.model(ex_image)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove noise for image and display results\n",
        "remove_noise(ex_image, res_image)\n"
      ],
      "metadata": {
        "id": "QBTSzuBwzuGM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DnCNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}